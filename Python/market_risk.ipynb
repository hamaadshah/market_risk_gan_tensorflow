{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "Copyright 2018 Hamaad Musharaf Shah\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bidirectional Generative Adversarial Networks to estimate Value-at-Risk for Market Risk Management\n",
    "## Author: Hamaad Shah\n",
    "\n",
    "---\n",
    "\n",
    "We will explore the use of Bidirectional Generative Adversarial Networks (BiGAN) for market risk management: Estimation of portfolio risk measures such as Value-at-Risk (VaR). Generative Adversarial Networks (GAN) allow us to implicitly maximize the likelihood of complex distributions thereby allowing us to generate samples from such distributions - the key point here is the implicit maximum likelihood estimation principle whereby we do not specify what this complex distribution is parameterized as. Dealing with high dimensional data potentially coming from a complex distribution is a key aspect to market risk management among many other financial services use cases. GAN, specifically BiGAN for the purpose of this paper, will allow us to deal with potentially complex financial services data such that we do not have to explicitly specify a distribution such as a multidimensional Gaussian distribution.\n",
    "\n",
    "## Market Risk Management: Value-at-Risk (VaR)\n",
    "\n",
    "---\n",
    "\n",
    "Let us begin by fetching a data set of 5 stocks from Yahoo. The stocks are Apple, Google, Microsoft, Intel and Box. We use a daily frequency for our data for the year 2016. We use the stock's daily closing prices to compute the continuously compounded returns: $\\log\\left(\\frac{V_{t+1}}{V_{t}}\\right) = \\log(V_{t+1}) - \\log(V_{t})$.\n",
    "\n",
    "```r\n",
    "# Set scientific notation penalty to high for better printing.\n",
    "options(scipen = 999)\n",
    "\n",
    "# Load required libraries.\n",
    "library(ggplot2)\n",
    "library(quantmod)\n",
    "library(MASS)\n",
    "library(scales)\n",
    "library(reshape2)\n",
    "\n",
    "# Let's take a year's worth of data.\n",
    "start <- as.Date(\"2016-01-01\")\n",
    "end <- as.Date(\"2016-12-31\")\n",
    "\n",
    "# Let's get Apple, Google, Microsoft, Intel and Box data from Yahoo.\n",
    "getSymbols(Symbols = \"AAPL;GOOG;MSFT;INTC;BOX\", \n",
    "           src = \"yahoo\", \n",
    "           from = start, \n",
    "           to = end)\n",
    "\n",
    "# Create daily returns series based on close prices.\n",
    "data <- data.frame(\"Apple\" = as.numeric(AAPL[, \"AAPL.Close\"]),\n",
    "                   \"Google\" = as.numeric(GOOG[, \"GOOG.Close\"]),\n",
    "                   \"Microsoft\" = as.numeric(MSFT[, \"MSFT.Close\"]),\n",
    "                   \"Intel\" = as.numeric(INTC[, \"INTC.Close\"]),\n",
    "                   \"Box\" = as.numeric(BOX[, \"BOX.Close\"]))\n",
    "ret.data <- data.frame(diff(x = as.matrix(log(x = data)), lag = 1))\n",
    "colnames(ret.data) <- c(\"Apple\", \"Google\", \"Microsoft\", \"Intel\", \"Box\")\n",
    "```\n",
    "\n",
    "Let's assume that we have an equal weight for each of the 5 assets in our portfolio. Based on this portfolio weights assumption we can calculate the portfolio returns.\n",
    "\n",
    "```r\n",
    "# Let's assume we have an equal weight for all assets in our 5 assets portfolio.\n",
    "# Based on those weightings calculate the portfolio returns on a daily basis.\n",
    "port.data <- data.frame(apply(X = ret.data, \n",
    "                              MARGIN = 1,\n",
    "                              FUN = function(x) weighted.mean(x = x, \n",
    "                                                              w = rep(x = 1 / ncol(ret.data), ncol(ret.data)))))\n",
    "colnames(port.data) <- \"Portfolio.ret\"\n",
    "\n",
    "# Plot the daily portfolio returns data.\n",
    "ggplot(data = port.data, \n",
    "       aes(x = Portfolio.ret)) +\n",
    "geom_density(fill = \"skyblue\", alpha = 0.25) +\n",
    "ggtitle(\"KDE plot of portfolio returns\") +\n",
    "xlab(\"Portfolio returns\") +\n",
    "ylab(\"Density\") +\n",
    "scale_x_continuous(labels = percent)\n",
    "```\n",
    "\n",
    "![](../R/portfolio_returns.png)\n",
    "\n",
    "Let's estimate the expected returns vector, volatilities vector, correlation and variance-covariance matrices. The variance-covariance matrix is recovered from the estimated volatilities vector and correlation matrix: $\\Omega = C \\odot \\sigma \\sigma^{T}$ where $\\odot$ is the Hadamard product, $C \\in \\mathbb{R}^{5 \\times 5}$ and $\\sigma \\in \\mathbb{R}^{5 \\times 1}$. Portfolio volatility is estimated as: $w^{T}\\Omega w$ where $w \\in \\mathbb{R}^{5 \\times 1}$\n",
    "\n",
    "```r\n",
    "# Get mean returns vector.\n",
    "est.ret.vector <- colMeans(x = ret.data)\n",
    "\n",
    "# Get volatilities vector.\n",
    "est.sd <- as.matrix(apply(X = ret.data, \n",
    "                          MARGIN = 2, \n",
    "                          FUN = sd))\n",
    "\n",
    "# Get correlation matrix.\n",
    "est.cor.matrix <- cor(x = ret.data)\n",
    "\n",
    "# Use volatilies vector and correlation matrix to estimate variance-covariance matrix.\n",
    "est.cov.matrix <- est.cor.matrix * (est.sd %*% t(est.sd))\n",
    "\n",
    "# Given the earlier portfolio weights estimate portfolio volatility.\n",
    "est.sd.port <- sqrt(t(as.matrix(rep(x = 1 / ncol(ret.data), ncol(ret.data)))) %*% \n",
    "                      est.cov.matrix %*% \n",
    "                      as.matrix(rep(x = 1 / ncol(ret.data), ncol(ret.data))))\n",
    "```\n",
    "\n",
    "We consider the 3 major methods used in market risk management, specifically for the estimation of VaR. Please note that there are multiple different methods for estimating VaR and other more coherent risk measures such as Conditional Value-at-Risk (CVaR) however we are only considering the few major ones.\n",
    "\n",
    "## VaR: Variance-covariance method\n",
    "\n",
    "---\n",
    "\n",
    "The first one is the variance-covariance method and uses the estimated portfolio volatility $w^{T}\\Omega w$ under the Gaussian assumption to estimate VaR. Let's assume we are attempting to estimate 1% VaR: This means that there is a 1% probability of obtaining a portfolio return of less than the VaR value. Using the variance-covariance approach the calculation is: $\\left[\\left(w^{T}\\Omega w\\right) \\mathcal{N}^{-1}(1\\%)\\right] + w^{T}\\mu$, where $\\mu \\in \\mathbb{R}^{5 \\times 1}$ is the expected returns vector.\n",
    "\n",
    "```r\n",
    "# Estimate VaR using variance-covariance method.\n",
    "VaR.var.covar <- (est.sd.port * qnorm(p = 0.01, \n",
    "                                      mean = 0, \n",
    "                                      sd = 1)) + mean(port.data$Portfolio.ret)\n",
    "```\n",
    "\n",
    "## VaR: Historical simulation method\n",
    "The second method is a non-parametric approach where we sample with replacement from the historical data to estimate a portfolio returns distribution. The 1% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n",
    "\n",
    "```r\n",
    "# Estimate VaR using historical simulation method.\n",
    "VaR.hist.sim <- quantile(x = sample(x = port.data$Portfolio.ret, \n",
    "                                    size = 1000, \n",
    "                                    replace = TRUE), \n",
    "                         probs = 0.01)\n",
    "```\n",
    "\n",
    "## VaR: Monte Carlo method\n",
    "\n",
    "---\n",
    "\n",
    "The third method is Monte Carlo sampling from a multidimensional Gaussian distribution using the aforementioned $mu$ and $\\Omega$ parameters. Finally the 1% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n",
    "\n",
    "```r\n",
    "# Estimate VaR using Monte Carlo simulation method.\n",
    "# Simulate returns data from multidimensional Gaussian distribution.\n",
    "sim.ret.data <- mvrnorm(n = 1000, \n",
    "                        mu = est.ret.vector, \n",
    "                        Sigma = est.cov.matrix)\n",
    "\n",
    "# Get portfolio returns.\n",
    "sim.port.data <- data.frame(apply(X = sim.ret.data, \n",
    "                                  MARGIN = 1,\n",
    "                                  FUN = function(x) weighted.mean(x = x, \n",
    "                                                                  w = rep(x = 1 / ncol(sim.ret.data), ncol(sim.ret.data)))))\n",
    "colnames(sim.port.data) <- \"Portfolio.ret\"\n",
    "\n",
    "VaR.MC <- quantile(x = sim.port.data$Portfolio.ret, \n",
    "                   probs = 0.01)\n",
    "```\n",
    "\n",
    "## VaR: Estimates\n",
    "The VaR estimates from the aforementioned 3 market risk management methods commonly used in banking are as follows:\n",
    "\n",
    "```r\n",
    "VaR <- data.frame(\"Var.Covar\" = VaR.var.covar,\n",
    "                  \"Hist.Sim\" = VaR.hist.sim,\n",
    "                  \"MC\" = VaR.MC)\n",
    "```\n",
    "\n",
    "| VaR Method    | 1% VaR | \n",
    "| :------------- |-------------:|\n",
    "| Variance-covariance | -2.87% | \n",
    "| Historical simulation | -3.65%  |\n",
    "| Monte Carlo simulation | -2.63%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Generative Adversarial Network (BiGAN)\n",
    "\n",
    "---\n",
    "\n",
    "The 2 main components to a Generative Adversarial Network (GAN) are the generator and the discriminator. These 2 components play an adversarial game against each other. In doing so the generator learns how to create realistic synthetic samples from noise, i.e., the latent space $z$, while the discriminator learns how to distinguish between a real sample and a synthetic sample. See the following article of mine for a detailed explanation of GAN: https://goo.gl/ZWYngw\n",
    "\n",
    "BiGAN extends GAN by adding a third component: The encoder, which learns to map from data space $x$ to the latent space $z$. The objective of the generator remains the same while the objective of the discriminator is altered to classify between a real sample and a synthetic sample and additionally between a real encoding, i.e., given by the encoder, and a synthetic encoding, i.e., a sample from the latent space $z$.\n",
    "\n",
    "### Generator\n",
    "\n",
    "---\n",
    "\n",
    "Assume that we have a prior belief on where the latent space $z$ lies: $p_{Z}(z)$. Given a draw from this latent space the generator $G$, a deep learner parameterized by $\\theta_{G}$, outputs a synthetic sample.\n",
    "\n",
    "$$\n",
    "G(z|\\theta_{G}): z \\rightarrow x_{synthetic}\n",
    "$$ \n",
    "\n",
    "### Encoder\n",
    "\n",
    "---\n",
    "\n",
    "This can be shown to be an inverse of the generator. Given a draw from the data space the encoder $E$, a deep learner parameterized by $\\theta_{E}$, outputs a real encoding.\n",
    "\n",
    "$$\n",
    "E(x|\\theta_{E}): x \\rightarrow z\n",
    "$$ \n",
    "\n",
    "### Discriminator\n",
    "\n",
    "---\n",
    "\n",
    "The discriminator $D$ is a deep learner parameterized by $\\theta_{D}$ and it aims to classify if a sample is real or synthetic, i.e., if a sample is from the real data distribution,\n",
    "\n",
    "$$\n",
    "p_{X}(x)\n",
    "$$ \n",
    "\n",
    "or the synthetic data distribution.\n",
    "\n",
    "$$\n",
    "p_{G}(x|z)\n",
    "$$\n",
    "\n",
    "Additionally it aims to classify whether an encoding is real,\n",
    "\n",
    "$$\n",
    "p_{E}(z|x)\n",
    "$$\n",
    "\n",
    "or synthetic.\n",
    "\n",
    "$$\n",
    "p_{Z}(z) \n",
    "$$\n",
    "\n",
    "Let us denote the discriminator $D$ as follows.\n",
    "\n",
    "$$\n",
    "D(\\{x, z\\}|\\theta_{D}): \\{x, z\\} \\rightarrow [0, 1]\n",
    "$$ \n",
    "\n",
    "We assume that the positive examples are real, i.e., $\\{x, E(x|\\theta_{E})\\}$ while the negative examples are synthetic, i.e., $\\{G(z|\\theta_{G}), z\\}$. \n",
    "\n",
    "### Optimal discriminator, encoder and generator\n",
    "\n",
    "---\n",
    "\n",
    "The BiGAN has the following objective function, similar to the GAN.\n",
    "\n",
    "$$\n",
    "\\min_{G(z|\\theta_{G}), E(x|\\theta_{E})} \\max_{D(\\{x, z\\}|\\theta_{D})} V(D(\\{x, z\\}|\\theta_{D}), G(z|\\theta_{G}), E(x|\\theta_{E}))\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "V(D(\\{x, z\\}|\\theta_{D}), G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\mathbb{E}_{x \\sim p_{X}(x)} \\mathbb{E}_{z \\sim p_{E}(z|x)} \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] + \\mathbb{E}_{z \\sim p_{Z}(z)} \\mathbb{E}_{x \\sim p_{G}(x|z)} \\log\\left[{1-D(\\{x, z\\}|\\theta_{D})}\\right] \\\\\n",
    "&= \\int_{x} p_{X}(x) \\int_{z} p_{E}(z|x) \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] dz dx + \\int_{z} p_{Z}(z) \\int_{x} p_{G}(x|z) \\log\\left[{1 - D(\\{x, z\\}|\\theta_{D})}\\right] dx dz \\\\\n",
    "&= \\int_{\\{x, z\\}} p_{X}(x) p_{E}(z|x) \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] d\\{x, z\\} + \\int_{\\{x, z\\}} p_{Z}(z) p_{G}(x|z) \\log\\left[{1 - D(\\{x, z\\}|\\theta_{D})}\\right] d\\{x, z\\} \\\\\n",
    "&= \\int_{\\omega:=\\{x, z\\}} \\underbrace{p_{EX}(\\omega) \\log\\left[{D(\\omega|\\theta_{D})}\\right] + p_{GZ}(\\omega) \\log\\left[{1 - D(\\omega|\\theta_{D})}\\right]}_{J(D(\\omega|\\theta_{D}))} d\\omega \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let us take a closer look at the discriminator's objective function for a sample $\\omega$.\n",
    "\n",
    "\\begin{align*}\n",
    "J(D(\\omega|\\theta_{D})) &= p_{EX}(\\omega) \\log{D(\\omega|\\theta_{D})} + p_{GZ}(\\omega) \\log{(1 - D(\\omega|\\theta_{D}))} \\\\\n",
    "\\frac{\\partial J(D(\\omega|\\theta_{D}))}{\\partial D(\\omega|\\theta_{D})} &= \\frac{p_{EX}(\\omega)}{D(\\omega|\\theta_{D})} - \\frac{p_{GZ}(\\omega)}{(1 - D(\\omega|\\theta_{D}))} \\\\\n",
    "0 &= \\frac{p_{EX}(\\omega)}{D^\\ast(\\omega|\\theta_{D^\\ast})} - \\frac{p_{GZ}(\\omega)}{(1 - D^\\ast(\\omega|\\theta_{D^\\ast}))} \\\\\n",
    "p_{EX}(\\omega)(1 - D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) \\\\\n",
    "p_{EX}(\\omega) - p_{EX}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) \\\\\n",
    "p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) + p_{EX}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{EX}(\\omega) \\\\\n",
    "D^\\ast(\\omega|\\theta_{D^\\ast}) &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)} \n",
    "\\end{align*}\n",
    "\n",
    "We have found the optimal discriminator given a generator and an encoder. Let us focus now on the generator and encoder's objective function which is essentially to minimize the discriminator's objective function.\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{D^\\ast(\\omega|\\theta_{D^\\ast})} + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{(1 - D^\\ast(\\omega|\\theta_{D^\\ast}))} \\\\\n",
    "&= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{\\bigg(1 - \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} \\\\\n",
    "&= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} \\\\\n",
    "&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega\n",
    "\\end{align*}\n",
    "\n",
    "We will note the Kullbackâ€“Leibler (KL) divergences in the above objective function for the generator and encoder.\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\int_{x} p(x) \\log\\bigg(\\frac{p(x)}{q(x)}\\bigg) dx\n",
    "$$\n",
    "\n",
    "Recall the definition of a $\\lambda$ divergence.\n",
    "\n",
    "$$\n",
    "D_{\\lambda}(P||Q) = \\lambda D_{KL}(P||\\lambda P + (1 - \\lambda) Q) + (1 - \\lambda) D_{KL}(Q||\\lambda P + (1 - \\lambda) Q)\n",
    "$$\n",
    "\n",
    "If $\\lambda$ takes the value of 0.5 this is then called the Jensen-Shannon (JS) divergence. This divergence is symmetric and non-negative.\n",
    "\n",
    "$$\n",
    "D_{JS}(P||Q) = 0.5 D_{KL}\\bigg(P\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg) + 0.5 D_{KL}\\bigg(Q\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg)\n",
    "$$\n",
    "\n",
    "Keeping this in mind let us take a look again at the objective function of the generator and the encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n",
    "&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg(\\frac{2}{2}\\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{2}{2}\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n",
    "&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n",
    "&= \\int_{\\omega} p_{EX}(\\omega) \\bigg[ \\log(0.5) + \\log{\\bigg(\\frac{p_{EX}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\ &+ \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log(0.5) + \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}\\bigg) \\bigg] } d\\omega \\\\\n",
    "&= \\log\\bigg(\\frac{1}{4}\\bigg) + \\int_{\\omega} p_{EX}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{EX}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\ \n",
    "&+ \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{GZ}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}\\bigg) \\bigg] } d\\omega \\\\\n",
    "&= -\\log(4) + D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) \\\\\n",
    "&= -\\log(4) + 2 \\bigg(0.5 D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + 0.5 D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg)\\bigg) \\\\\n",
    "&= -\\log(4) + 2D_{JS}(P_{EX}||P_{GZ}) \n",
    "\\end{align*}\n",
    "\n",
    "It is clear from the objective function of the generator and encoder above that the global minimum value attained is $-\\log(4)$ which occurs when the following holds.\n",
    "\n",
    "$$\n",
    "P_{EX}=P_{GZ}\n",
    "$$\n",
    "\n",
    "When the above holds the Jensen-Shannon divergence, i.e., $D_{JS}(P_{EX}||P_{GZ})$, will be zero. Hence we have shown that the optimal solution is as follows.\n",
    "\n",
    "$$\n",
    "P_{EX}=P_{GZ}\n",
    "$$\n",
    "\n",
    "Given the above result we can prove that the optimal discriminator will be $\\frac{1}{2}$.\n",
    "\n",
    "\\begin{align*}\n",
    "D^\\ast(\\omega|\\theta_{D^\\ast}) &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)} \\\\\n",
    " &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{EX}(\\omega)} \\\\\n",
    " &= \\frac{p_{EX}(\\omega)}{2p_{EX}(\\omega)} \\\\\n",
    " &= \\frac{1}{2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Optimal encoder and generator are inverse functions of each other\n",
    "\n",
    "---\n",
    "\n",
    "At the optimal generator and encoder we can show that the generator and encoder are inverse functions of each other. Recall from earlier the definitions of the generator and the encoder.\n",
    "\n",
    "$$\n",
    "G(z|\\theta_{G}): z \\rightarrow x_{synthetic}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "E(x|\\theta_{E}): x \\rightarrow z\n",
    "$$ \n",
    "\n",
    "At this point the optimal discriminator is $\\frac{1}{2}$, i.e., the discriminator cannot effectively differentiate between real and synthetic data as the synthetic data is realistic. Remember that at this point the likelihood would have been implicitly maximized such that any samples taken from the synthetic distribution should be similar to those taken from the real distribution. In short, if optimality of the generator, encoder and discriminator holds then the synthetic data should look similar, or rather be the same, as the real data. Keeping this important point in mind let's slightly re-write the optimal generator and encoder functions.\n",
    "\n",
    "$$\n",
    "G^\\ast(z|\\theta_{G^\\ast}): z \\rightarrow x\n",
    "$$ \n",
    "\n",
    "$$\n",
    "E^\\ast(x|\\theta_{E^\\ast}): x \\rightarrow z\n",
    "$$ \n",
    "\n",
    "Recall further that the following holds at the optimal generator and encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX} &= \\int_{x} p_{X}(x) \\int_{z=E^\\ast(x|\\theta_{E^\\ast})} p_{E^\\ast}(z|x) dz dx \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the above please note the following; note also that we make the assumption that the generator is not an inverse function of the encoder for providing a proof by contradiction.\n",
    "\n",
    "\\begin{align*}\n",
    "z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n",
    "x&\\neq G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Recall that optimality condition of the generator and encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX} &= P_{GZ} \\\\\n",
    "P_{GZ} &= \\int_{z} p_{Z}(z) \\int_{x=G^\\ast(z|\\theta_{G^\\ast})} p_{G^\\ast}(x|z) dx dz \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the above please note the following.\n",
    "\n",
    "\\begin{align*}\n",
    "x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n",
    "z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n",
    "z&=E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "G^\\ast(z|\\theta_{G^\\ast})&=G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "If optimality holds then the following holds as shown above.\n",
    "\n",
    "$$\n",
    "G^\\ast(z|\\theta_{G^\\ast})=G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast})\n",
    "$$\n",
    "\n",
    "However since we assumed that the generator is not an inverse function of the encoder then the above conditions cannot hold thereby violating the optimality condition.\n",
    "\n",
    "\\begin{align*}\n",
    "x&\\neq G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "G^\\ast(z|\\theta_{G^\\ast})&\\neq G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore we have shown by contradiction that under optimality of the generator and encoder the generator is an inverse function of the encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "x&=G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The same arguments made above can be shown for the encoder being the inverse of the generator.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ} &= \\int_{z} p_{Z}(z) \\int_{x=G^\\ast(z|\\theta_{G^\\ast})} p_{G^\\ast}(x|z) dx dz \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the above please note the following; note also that we make the assumption that the encoder is not an inverse function of the generator for providing a proof by contradiction.\n",
    "\n",
    "\\begin{align*}\n",
    "x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n",
    "z&\\neq E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Recall that optimality condition of the generator and encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX} &= P_{GZ} \\\\\n",
    "P_{EX} &= \\int_{x} p_{X}(x) \\int_{z=E^\\ast(x|\\theta_{E^\\ast})} p_{E^\\ast}(z|x) dz dx \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the above please note the following.\n",
    "\n",
    "\\begin{align*}\n",
    "z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n",
    "x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n",
    "x&=G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n",
    "E^\\ast(x|\\theta_{E^\\ast})&=E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "If optimality holds then the following holds as shown above.\n",
    "\n",
    "$$\n",
    "E^\\ast(x|\\theta_{E^\\ast})=E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast})\n",
    "$$\n",
    "\n",
    "However since we assumed that the encoder is not an inverse function of the generator then the above conditions cannot hold thereby violating the optimality condition.\n",
    "\n",
    "\\begin{align*}\n",
    "z&\\neq E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "E^\\ast(x|\\theta_{E^\\ast})&\\neq E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore we have shown by contradiction that under optimality of the generator and encoder the encoder is an inverse function of the generator.\n",
    "\n",
    "\\begin{align*}\n",
    "z&= E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore we have shown that the optimal encoder and generator are inverse functions of each other via proof by contradiction: If they were not inverse functions of each other then it would violate the optimality condition for the encoder and generator, i.e.,  $P_{EX} = P_{GZ}$.\n",
    "\n",
    "### BiGAN relation to autoencoders\n",
    "\n",
    "At this point it might be a good idea to review my previous article on autoencoders here: https://goo.gl/qWqbbv\n",
    "\n",
    "Note that given an optimal discriminator, the objective function of the generator and encoder can be thought of as that of an autoencoder, where the generator plays the role of a decoder. The objective function of the generator and encoder is simply to minimize the objective function of the discriminator, i.e., we have not explicitly specified the structure of the reconstruction loss as one might do so with an autoencoder. This implicit minimization of the reconstruction loss is yet another great advantage of BiGAN: One does not need to explicitly define a reconstruction loss. \n",
    "\n",
    "Let's remind ourselves of the objective function of the generator and encoder.\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= -\\log(4) + 2D_{JS}(P_{EX}||P_{GZ}) \\\\\n",
    "&= -\\log(4) + 2 \\bigg(0.5 D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + 0.5 D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg)\\bigg) \\\\\n",
    "&= -\\log(4) + D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) \\\\\n",
    "&= \\log\\bigg(\\frac{1}{4}\\bigg) + \\int_{\\omega} p_{EX}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{EX}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\ \n",
    "&+ \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{GZ}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}\\bigg) \\bigg] } d\\omega \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let's deal with $F_{EG}$ first and then with $F_{GE}$ second in a similar manner. These are defined as follows.\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) &= \\underbrace{\\log{2}}_{\\text{We can omit this constant for simplicity}} + \\int_{\\omega} p_{EX}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{EX}(\\omega)}{(p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\\n",
    "D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) - \\log{2} &= \\int_{\\omega} \\bigg[\\log{\\bigg(\\frac{p_{EX}(\\omega)d\\omega}{\\left[\\underbrace{(p_{EX}(\\omega) + p_{GZ}(\\omega))d\\omega}_{d(P_{EX} + P_{GZ})}\\right]}} \\bigg) \\bigg] \\underbrace{p_{EX}(\\omega) d\\omega}_{dP_{EX}} \\\\\n",
    "F_{EG} &= \\int \\bigg[\\log{\\underbrace{\\bigg(\\frac{dP_{EX}}{d(P_{EX} + P_{GZ})}\\bigg)}_{\\text{Radon-Nikodym derivative: }f_{EG}}} \\bigg] dP_{EX} \\\\\n",
    "F_{EG} &= \\int \\log{f_{EG}} dP_{EX} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Briefly recall the definition of a Radon-Nikodym derivative: $\\frac{d\\nu}{d\\mu}$.\n",
    "\n",
    "$$\n",
    "\\nu = \\int \\frac{d\\nu}{d\\mu}d\\mu\n",
    "$$\n",
    "\n",
    "It follows then.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX}(X) &= \\int_{X} \\frac{dP_{EX}}{d(P_{EX} + P_{GZ})} d(P_{EX} + P_{GZ}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now for the second term: $F_{GE}$.\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) &= \\underbrace{\\log{2}}_{\\text{We can omit this constant for simplicity}} + \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{GZ}(\\omega)}{(p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\\n",
    "D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) - \\log{2} &= \\int_{\\omega} \\bigg[\\log{\\bigg(\\frac{p_{GZ}(\\omega)d\\omega}{\\left[\\underbrace{(p_{EX}(\\omega) + p_{GZ}(\\omega))d\\omega}_{d(P_{EX} + P_{GZ})}\\right]}} \\bigg) \\bigg] \\underbrace{p_{GZ}(\\omega) d\\omega}_{dP_{GZ}} \\\\\n",
    "F_{GE} &= \\int \\bigg[\\log{\\underbrace{\\bigg(\\frac{dP_{GZ}}{d(P_{EX} + P_{GZ})}\\bigg)}_{\\text{Radon-Nikodym derivative: }f_{GE}}} \\bigg] dP_{GZ} \\\\\n",
    "F_{GE} &= \\int \\log{f_{GE}} dP_{GZ} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "It follows then.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ}(X) &= \\int_{X} \\frac{dP_{GZ}}{d(P_{EX} + P_{GZ})} d(P_{EX} + P_{GZ}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note also that.\n",
    "\n",
    "\\begin{align*}\n",
    "f_{EG} + f_{GE} &= \\frac{dP_{EX}}{d(P_{EX} + P_{GZ})} + \\frac{dP_{GZ}}{d(P_{EX} + P_{GZ})} \\\\\n",
    "&= \\frac{dP_{EX} + dP_{GZ}}{d(P_{EX} + P_{GZ})} \\\\\n",
    "&= \\frac{dP_{EX} + dP_{GZ}}{dP_{EX} + dP_{GZ}} \\\\\n",
    "&= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now we shall prove that $f_{EG} > 0$ with $P_{EX}$ almost everywhere. To prove this, assume that $f_{EG} = 0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX}(X) &= \\int_{X} f_{EG} d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{EX}(X) &= \\int_{X} 0 d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{EX}(X) &= 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $f_{EG} > 0$ with $P_{EX}$ almost everywhere. This means that $F_{EG}$ is well defined.\n",
    "\n",
    "Now we shall prove that $f_{GE} > 0$ with $P_{GZ}$ almost everywhere. To prove this, assume that $f_{GE} = 0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ}(X) &= \\int_{X} f_{GE} d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{GZ}(X) &= \\int_{X} 0 d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{GZ}(X) &= 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $f_{GE} > 0$ with $P_{GZ}$ almost everywhere. This means that $F_{GE}$ is well defined.\n",
    "\n",
    "The $F_{EG}$ outside the support of $P_{GZ}$ is $0$. Similarly, the $F_{GE}$ outside the support of $P_{EX}$ is $0$. We start with the former.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) &= \\int_{\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}} f_{EG} d(P_{EX} + P_{GZ}) \\\\\n",
    "&= \\int_{\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}} f_{EG} dP_{EX} + \\int_{\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}} f_{EG} dP_{GZ} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that because of $\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}$ this implies $dP_{GZ}=p_{GZ}d\\omega=0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) &= \\int_{\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}} f_{EG} dP_{EX} + \\underbrace{\\int_{\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}} f_{EG} dP_{GZ}}_{\\text{This term is } 0 \\text{ due to the aforementioned note}} \\\\\n",
    "&\\leq \\underbrace{\\epsilon}_{f_{EG} \\leq \\epsilon < 1} P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) \\\\\n",
    "&< P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that in the above proof $\\epsilon$ is simply a term that is at least as large or larger than $f_{EG}$ while being smaller than $1$. Assuming this holds we clearly show that $P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) < P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\})$ which is a contradiction. Hence, $P_{EX}(\\{X\\text{\\supp}(P_{GZ}) | f_{EG} < 1\\}) = 0$ and $f_{EG}=1$ almost everywhere in $X\\text{\\supp}(P_{GZ})$. This implies that $\\log{f_{EG}}=0 \\text{, } P_{EX}$ almost everywhere in $X\\text{\\supp}(P_{GZ})$. Hence  $F_{EG}$ in the support $X\\text{\\supp}(P_{GZ})$ is $0$, i.e., $F_{EG}$ outside the support of $P_{GZ}$ is $0$.\n",
    "\n",
    "The $F_{GE}$ outside the support of $P_{EX}$ is $0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) &= \\int_{\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}} f_{GE} d(P_{EX} + P_{GZ}) \\\\\n",
    "&= \\int_{\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}} f_{GE} dP_{GZ} + \\int_{\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}} f_{GE} dP_{EX} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that because of $\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}$ this implies $dP_{EX}=p_{EX}d\\omega=0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) &= \\int_{\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}} f_{GE} dP_{GZ} + \\underbrace{\\int_{\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}} f_{GE} dP_{EX}}_{\\text{This term is } 0 \\text{ due to the aforementioned note}} \\\\\n",
    "&\\leq \\underbrace{\\epsilon}_{f_{GE} \\leq \\epsilon < 1} P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) \\\\\n",
    "&< P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that in the above proof $\\epsilon$ is simply a term that is at least as large or larger than $f_{GE}$ while being smaller than $1$. Assuming this holds we clearly show that $P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) < P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\})$ which is a contradiction. Hence, $P_{GZ}(\\{X\\text{\\supp}(P_{EX}) | f_{GE} < 1\\}) = 0$ and $f_{GE}=1$ almost everywhere in $X\\text{\\supp}(P_{EX})$. This implies that $\\log{f_{GE}}=0 \\text{, } P_{GZ}$ almost everywhere in $X\\text{\\supp}(P_{EX})$. Hence $F_{GE}$ in the support $X\\text{\\supp}(P_{EX})$ is $0$, i.e., $F_{GE}$ outside the support of $P_{EX}$ is $0$.\n",
    "\n",
    "Therefore the aforementioned KL divergences are most likely non-zero in $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$. In this space we show that $f_{EG}<1$, $P_{EX}$ almost everywhere and $f_{GE}<1$, $P_{GZ}$ almost everywhere.\n",
    "\n",
    "Lets assume that in the region $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$, $f_{EG}=1$ and that this set $\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}$ is not empty. This implies that $P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\})>0$ and $P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\})>0$. Therefore the following holds.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}) &= \\int_{\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}} f_{EG} d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}) &= \\int_{\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}} 1 d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}) &= P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}) + P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Clearly the above implies that $P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\})=0$ and contradicts the earlier definition of support, i.e., $P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\})>0$. Hence $\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{EG}=1\\}$ is an empty set and hence $f_{EG}<1$, $P_{EX}$ almost everywhere on $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$. Note finally that this implies that $\\log{f_{EG}}<0$, $P_{EX}$ almost everywhere.\n",
    "\n",
    "Lets assume that in the region $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$, $f_{GE}=1$ and that this set $\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}$ is not empty. This implies that $P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\})>0$ and $P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\})>0$. Therefore the following holds.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}) &= \\int_{\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}} f_{GE} d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}) &= \\int_{\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}} 1 d(P_{EX} + P_{GZ}) \\\\\n",
    "P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}) &= P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}) + P_{GZ}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Clearly the above implies that $P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\})=0$ and contradicts the earlier definition of support, i.e., $P_{EX}(\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\})>0$. Hence $\\{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ}) | f_{GE}=1\\}$ is an empty set and hence $f_{GE}<1$, $P_{GZ}$ almost everywhere on $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$. Note finally that this implies that $\\log{f_{GE}}<0$, $P_{GZ}$ almost everywhere.\n",
    "\n",
    "We have clearly shown that $F_{EG}(X\\text{\\supp}(P_{GZ}))=0$ and $F_{GE}(X\\text{\\supp}(P_{EX}))=0$. This implies that $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$ is the only region where $F_{EG}$ and $F_{GE}$ might be non-zero.\n",
    "\n",
    "Therefore.\n",
    "\n",
    "\\begin{align*}\n",
    "F_{EG} &= \\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{f_{EG}} dP_{EX} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "F_{GE} &= \\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{f_{GE}} dP_{GZ} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= F_{EG} + F_{GE} \\\\\n",
    "&= \\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{f_{EG}} dP_{EX} + \\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{f_{GE}} dP_{GZ} \\\\\n",
    "&= \\underbrace{\\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{f_{EG}} dP_{EX} + \\int_{\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})} \\log{(1 - f_{EG})} dP_{GZ}}_{\\text{Note that this is an autoencoder type reconstruction loss function: We find an encoder and generator such that it minimizes this function given an optimal discriminator}} \\\\\n",
    "&= D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) - \\log{2} + D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) - \\log{2} \\\\\n",
    "&= D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) - \\log{4} \n",
    "\\end{align*}\n",
    "\n",
    "We have clearly shown that $f_{EG} \\in (0, 1)$, $P_{EX}$ almost everywhere in $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$. Therefore, $\\log{f_{EG}} \\in (-\\inf, 0)$, $P_{EX}$ almost everywhere.\n",
    "\n",
    "We have clearly shown that $f_{GE} \\in (0, 1)$, $P_{GZ}$ almost everywhere in $\\text{supp}(P_{EX})\\cap\\text{supp}(P_{GZ})$. Therefore, $\\log{f_{GE}} \\in (-\\inf, 0)$, $P_{GZ}$ almost everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# License\n",
    "# Copyright 2018 Hamaad Musharaf Shah\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling, Reshape, Embedding\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# BiGAN implementation inspired from here: https://github.com/eriklindernoren/Keras-GAN\n",
    "# I believe I can re-write it slightly differently. On my to-do list.\n",
    "class BiGAN(BaseEstimator, \n",
    "            TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 z_size=None,\n",
    "                 iterations=None,\n",
    "                 batch_size=None):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "        \n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "            \n",
    "        # Build the discriminator.\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=RMSprop(lr=0.0002, \n",
    "                                                     clipvalue=1.0,\n",
    "                                                     decay=1e-8),\n",
    "                                   loss=\"binary_crossentropy\",\n",
    "                                   metrics=[\"accuracy\"])\n",
    "\n",
    "        # Build the generator to fool the discriminator.\n",
    "        # Freeze the discriminator here.\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator = self.build_generator()\n",
    "        self.encoder = self.build_encoder()\n",
    "        \n",
    "        z = Input(shape=(self.z_size, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        img = Input(shape=(28, 28, 1))\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid.\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model.\n",
    "        # Trains generator to fool the discriminator.\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n",
    "                                     optimizer=RMSprop(lr=0.0004, \n",
    "                                                       clipvalue=1.0,\n",
    "                                                       decay=1e-8))\n",
    " \n",
    "    def fit(self,\n",
    "            X,\n",
    "            y=None):\n",
    "        num_train = X.shape[0]\n",
    "        start = 0\n",
    "        \n",
    "        # Adversarial ground truths.\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "        \n",
    "        for step in range(self.iterations):\n",
    "            # Generate a new batch of noise...\n",
    "            noise = np.random.uniform(low=-1.0, high=1.0, size=(self.batch_size, self.z_size))\n",
    "            # ...and generate a batch of fake images.\n",
    "            generated_images = self.generator.predict(noise)\n",
    "            \n",
    "            stop = start + self.batch_size\n",
    "            # Get a batch of real images.\n",
    "            image_batch = X[start:stop]\n",
    "            # Encoder these real images.\n",
    "            encoding = self.encoder.predict(image_batch)\n",
    "\n",
    "            # Train the discriminator (image_batch -> encoding is valid, noise -> generated_image is fake).\n",
    "            d_loss_real = self.discriminator.train_on_batch([encoding, image_batch], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([noise, generated_images], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train the generator (noise -> generated_image is valid and image_batch -> encoding is is invalid).\n",
    "            g_loss = self.bigan_generator.train_on_batch([noise, image_batch], [valid, fake])\n",
    "            \n",
    "            start += self.batch_size\n",
    "            if start > num_train - self.batch_size:\n",
    "                start = 0\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                # Plot the progress.\n",
    "                print (\"[D loss: %f, acc: %.2f%%] [G loss: %f]\" % (d_loss[0], 100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "                img = image.array_to_img(generated_images[0] * 255.0, scale=False)\n",
    "                img.save(\"outputs/generated_image\" + str(step) + \".png\")\n",
    "                \n",
    "                img = image.array_to_img(image_batch[0] * 255.0, scale=False)\n",
    "                img.save(\"outputs/real_image\" + str(step) + \".png\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self,\n",
    "                  X):\n",
    "        return self.feature_extractor.predict(X)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        encoder_input = Input(shape=(28, 28, 1))\n",
    "\n",
    "        encoder_model = Flatten()(encoder_input)\n",
    "        encoder_model = Dense(units=512)(encoder_model)\n",
    "        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n",
    "        encoder_model = BatchNormalization(momentum=0.8)(encoder_model)\n",
    "        encoder_model = Dense(units=512)(encoder_model)\n",
    "        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n",
    "        encoder_model = BatchNormalization(momentum=0.8)(encoder_model)\n",
    "        \n",
    "        encoder_output = Dense(units=self.z_size)(encoder_model)\n",
    "        self.feature_extractor = Model(encoder_input, encoder_output)\n",
    "        \n",
    "        return Model(encoder_input, encoder_output)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        # We will map z, a latent vector, to image space (..., 28, 28, 1).\n",
    "        latent = Input(shape=(self.z_size,))\n",
    "\n",
    "        # This produces a (..., 7, 7, 128) shaped tensor.\n",
    "        cnn = Dense(units=1024, activation=\"tanh\")(latent)\n",
    "        cnn = Dense(units=128 * 7 * 7, activation=\"tanh\")(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = Reshape((7, 7, 128))(cnn)\n",
    "\n",
    "        # Upsample to (..., 14, 14, 64).\n",
    "        cnn = layers.Conv2DTranspose(filters=64, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "        cnn = layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "\n",
    "        # Upsample to (..., 28, 28, 64).\n",
    "        cnn = layers.Conv2DTranspose(filters=64, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "\n",
    "        # Take a channel axis reduction to (..., 28, 28, 1).\n",
    "        fake_img = Conv2D(filters=1, kernel_size=(5, 5), strides=(1, 1), padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"glorot_normal\", name=\"generator\")(cnn)\n",
    "\n",
    "        return Model(latent, fake_img)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        z = Input(shape=(self.z_size,))\n",
    "        image = Input(shape=(28, 28, 1))\n",
    "        discriminator_inputs = concatenate([z, Flatten()(image)], axis=1)\n",
    "\n",
    "        discriminator_model = Dense(1024)(discriminator_inputs)\n",
    "        discriminator_model = LeakyReLU(alpha=0.2)(discriminator_model)\n",
    "        discriminator_model = Dropout(0.5)(discriminator_model)\n",
    "        discriminator_model = Dense(1024)(discriminator_model)\n",
    "        discriminator_model = LeakyReLU(alpha=0.2)(discriminator_model)\n",
    "        discriminator_model = Dropout(0.5)(discriminator_model)\n",
    "        discriminator_model = Dense(1024)(discriminator_model)\n",
    "        discriminator_model = LeakyReLU(alpha=0.2)(discriminator_model)\n",
    "        discriminator_model = Dropout(0.5)(discriminator_model)\n",
    "        \n",
    "        discriminator_output = Dense(1, activation=\"sigmoid\")(discriminator_model)\n",
    "        \n",
    "        return Model([z, image], discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10380253630489579796\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "from PIL import Image\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling, Reshape, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotnine import *\n",
    "import plotnine\n",
    "\n",
    "get_ipython().magic(\"matplotlib inline\")\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "importlib.reload(bkend)\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "mnist = mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist\n",
    "x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1], x_train.shape[2], 1])\n",
    "x_test = np.reshape(x_test, [x_test.shape[0], x_test.shape[1], x_test.shape[2], 1])\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "scaler_classifier = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "logistic = linear_model.LogisticRegression(random_state=666, verbose=1)\n",
    "lb = LabelBinarizer()\n",
    "lb = lb.fit(y_train.reshape(y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BiGAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-26d85d452ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bigan = BiGAN(z_size=5,\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               iterations=10000)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe_dcgan = Pipeline(steps=[(\"DCGAN\", bigan),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BiGAN' is not defined"
     ]
    }
   ],
   "source": [
    "bigan = BiGAN(z_size=2,\n",
    "              batch_size=100,\n",
    "              iterations=10000)\n",
    "\n",
    "pipe_dcgan = Pipeline(steps=[(\"DCGAN\", bigan),\n",
    "                             (\"scaler_classifier\", scaler_classifier),\n",
    "                             (\"classifier\", logistic)])\n",
    "pipe_dcgan = pipe_dcgan.fit(x_train, y_train)\n",
    "\n",
    "acc_dcgan = pipe_dcgan.score(x_test, y_test)\n",
    "\n",
    "print(\"The accuracy score for the MNIST classification task with DCGAN: %.6f%%.\" % (acc_dcgan * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>Intel</th>\n",
       "      <th>Box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>2.510000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.096240e-18</td>\n",
       "      <td>-1.946208e-17</td>\n",
       "      <td>-1.503888e-17</td>\n",
       "      <td>1.592352e-17</td>\n",
       "      <td>-2.277948e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001998e+00</td>\n",
       "      <td>1.001998e+00</td>\n",
       "      <td>1.001998e+00</td>\n",
       "      <td>1.001998e+00</td>\n",
       "      <td>1.001998e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.635540e+00</td>\n",
       "      <td>-4.366264e+00</td>\n",
       "      <td>-5.243146e+00</td>\n",
       "      <td>-6.646280e+00</td>\n",
       "      <td>-4.477044e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.304943e-01</td>\n",
       "      <td>-4.054559e-01</td>\n",
       "      <td>-4.564779e-01</td>\n",
       "      <td>-4.579187e-01</td>\n",
       "      <td>-3.896675e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.567043e-02</td>\n",
       "      <td>4.681882e-03</td>\n",
       "      <td>-1.014329e-02</td>\n",
       "      <td>7.532364e-02</td>\n",
       "      <td>6.227469e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.950689e-01</td>\n",
       "      <td>5.999133e-01</td>\n",
       "      <td>4.661762e-01</td>\n",
       "      <td>5.722558e-01</td>\n",
       "      <td>5.501637e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.243533e+00</td>\n",
       "      <td>3.436666e+00</td>\n",
       "      <td>3.924404e+00</td>\n",
       "      <td>2.373759e+00</td>\n",
       "      <td>2.942306e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Apple        Google     Microsoft         Intel           Box\n",
       "count  2.510000e+02  2.510000e+02  2.510000e+02  2.510000e+02  2.510000e+02\n",
       "mean  -3.096240e-18 -1.946208e-17 -1.503888e-17  1.592352e-17 -2.277948e-17\n",
       "std    1.001998e+00  1.001998e+00  1.001998e+00  1.001998e+00  1.001998e+00\n",
       "min   -4.635540e+00 -4.366264e+00 -5.243146e+00 -6.646280e+00 -4.477044e+00\n",
       "25%   -4.304943e-01 -4.054559e-01 -4.564779e-01 -4.579187e-01 -3.896675e-01\n",
       "50%    2.567043e-02  4.681882e-03 -1.014329e-02  7.532364e-02  6.227469e-02\n",
       "75%    4.950689e-01  5.999133e-01  4.661762e-01  5.722558e-01  5.501637e-01\n",
       "max    4.243533e+00  3.436666e+00  3.924404e+00  2.373759e+00  2.942306e+00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_data = pd.read_csv(filepath_or_buffer=\"../R/ret_data.csv\")\n",
    "mean = ret_data.apply(func=np.mean, axis=0)\n",
    "ret_data -= mean\n",
    "std = ret_data.apply(func=np.std, axis=0)\n",
    "ret_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# License\n",
    "# Copyright 2018 Hamaad Musharaf Shah\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling, Reshape, Embedding\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# BiGAN implementation inspired from here: https://github.com/eriklindernoren/Keras-GAN\n",
    "# I believe I can re-write it slightly differently. On my to-do list.\n",
    "class BiGAN(BaseEstimator, \n",
    "            TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 z_size=None,\n",
    "                 iterations=None,\n",
    "                 batch_size=None):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "        \n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "            \n",
    "        # Build the discriminator.\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer=RMSprop(lr=0.0002, \n",
    "                                                     clipvalue=1.0,\n",
    "                                                     decay=1e-8),\n",
    "                                   loss=\"binary_crossentropy\",\n",
    "                                   metrics=[\"accuracy\"])\n",
    "\n",
    "        # Build the generator to fool the discriminator.\n",
    "        # Freeze the discriminator here.\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator = self.build_generator()\n",
    "        self.encoder = self.build_encoder()\n",
    "        \n",
    "        noise = Input(shape=(self.z_size, ))\n",
    "        generated_data = self.generator(noise)\n",
    "\n",
    "        real_data = Input(shape=(5,))\n",
    "        encoding = self.encoder(real_data)\n",
    "\n",
    "        # Latent -> returns data is fake, and returns data -> latent is valid.\n",
    "        fake = self.discriminator([noise, generated_data])\n",
    "        valid = self.discriminator([encoding, real_data])\n",
    "\n",
    "        # Set up and compile the combined model.\n",
    "        # Trains generator to fool the discriminator.\n",
    "        self.bigan_generator = Model([noise, real_data], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n",
    "                                     optimizer=RMSprop(lr=0.0004, \n",
    "                                                       clipvalue=1.0,\n",
    "                                                       decay=1e-8))\n",
    " \n",
    "    def fit(self,\n",
    "            X,\n",
    "            y=None):\n",
    "        num_train = X.shape[0]\n",
    "        start = 0\n",
    "        \n",
    "        # Adversarial ground truths.\n",
    "        valid = np.ones((self.batch_size, 1)) \n",
    "        fake = np.zeros((self.batch_size, 1))        \n",
    "        \n",
    "        for step in range(self.iterations):\n",
    "            # Generate a new batch of noise...\n",
    "            noise = np.random.uniform(low=-1.0, high=1.0, size=(self.batch_size, self.z_size))\n",
    "            # ...and generate a batch of synthetic returns data.\n",
    "            generated_data = self.generator.predict(noise)\n",
    "            \n",
    "            stop = start + self.batch_size\n",
    "            # Get a batch of real returns data.\n",
    "            real_batch = X[start:stop]\n",
    "            # Encoder these real images.\n",
    "            encoding = self.encoder.predict(real_batch)\n",
    "\n",
    "            # Train the discriminator (real_batch -> encoding is valid, noise -> generated_data is fake).\n",
    "            d_loss_real = self.discriminator.train_on_batch([encoding, real_batch], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([noise, generated_data], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train the generator (noise -> generated_data is valid and real_batch -> encoding is is invalid).\n",
    "            g_loss = self.bigan_generator.train_on_batch([noise, real_batch], [valid, fake])\n",
    "            \n",
    "            start += self.batch_size\n",
    "            if start > num_train - self.batch_size:\n",
    "                start = 0\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                # Plot the progress.\n",
    "                print (\"[D loss: %f, acc: %.2f%%] [G loss: %f]\" % (d_loss[0], 100 * d_loss[1], g_loss[0]))\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def transform(self,\n",
    "                  X):\n",
    "        return self.feature_extractor.predict(X)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        encoder_input = Input(shape=(5,))\n",
    "\n",
    "        encoder_model = Dense(units=100)(encoder_input)\n",
    "        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n",
    "        encoder_model = BatchNormalization(momentum=0.8)(encoder_model)\n",
    "        encoder_model = Dense(units=100)(encoder_input)\n",
    "        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n",
    "        encoder_model = BatchNormalization(momentum=0.8)(encoder_model)\n",
    "        encoder_model = Dense(units=100)(encoder_input)\n",
    "        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n",
    "        \n",
    "        encoder_output = Dense(units=self.z_size)(encoder_model)\n",
    "        \n",
    "        self.feature_extractor = Model(encoder_input, encoder_output)\n",
    "        \n",
    "        return Model(encoder_input, encoder_output)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        # We will map z, a latent vector, to continuous returns data space (..., 5).\n",
    "        latent = Input(shape=(self.z_size,))\n",
    "\n",
    "        # This produces a (..., 100) shaped tensor.\n",
    "        generator_model = Dense(units=100, activation=\"elu\")(latent)\n",
    "        generator_model = BatchNormalization()(generator_model)\n",
    "        generator_model = Dropout(0.5)(generator_model)\n",
    "        generator_model = Dense(units=100, activation=\"elu\")(generator_model)\n",
    "        generator_model = BatchNormalization()(generator_model)\n",
    "        generator_model = Dropout(0.5)(generator_model)\n",
    "        generator_model = Dense(units=100, activation=\"elu\")(generator_model)\n",
    "        \n",
    "        generator_output = Dense(units=5, activation=\"linear\")(generator_model)\n",
    "        \n",
    "        return Model(latent, generator_output)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        z = Input(shape=(self.z_size,))\n",
    "        ret_data = Input(shape=(5,))\n",
    "        discriminator_inputs = concatenate([z, ret_data], axis=1)\n",
    "\n",
    "        discriminator_model = Dense(100)(discriminator_inputs)\n",
    "        discriminator_model = LeakyReLU(alpha=0.2)(discriminator_model)\n",
    "        discriminator_model = Dropout(0.5)(discriminator_model)\n",
    "\n",
    "        discriminator_output = Dense(1, activation=\"sigmoid\")(discriminator_model)\n",
    "        \n",
    "        return Model([z, ret_data], discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.735398, acc: 42.03%] [G loss: 1.586014]\n",
      "[D loss: 1.218448, acc: 26.29%] [G loss: 1.083077]\n",
      "[D loss: 1.028279, acc: 30.68%] [G loss: 1.191478]\n",
      "[D loss: 0.964117, acc: 32.27%] [G loss: 1.265626]\n",
      "[D loss: 0.859315, acc: 36.45%] [G loss: 1.356389]\n",
      "[D loss: 0.860158, acc: 37.85%] [G loss: 1.302269]\n",
      "[D loss: 0.802230, acc: 38.65%] [G loss: 1.364189]\n",
      "[D loss: 0.800437, acc: 35.26%] [G loss: 1.305795]\n",
      "[D loss: 0.788200, acc: 38.25%] [G loss: 1.332470]\n",
      "[D loss: 0.780504, acc: 35.66%] [G loss: 1.347013]\n",
      "[D loss: 0.741290, acc: 44.42%] [G loss: 1.394239]\n",
      "[D loss: 0.749538, acc: 42.83%] [G loss: 1.397665]\n",
      "[D loss: 0.759217, acc: 41.63%] [G loss: 1.375918]\n",
      "[D loss: 0.761937, acc: 38.65%] [G loss: 1.357667]\n",
      "[D loss: 0.741930, acc: 38.25%] [G loss: 1.365476]\n",
      "[D loss: 0.742958, acc: 40.04%] [G loss: 1.370959]\n",
      "[D loss: 0.733580, acc: 39.84%] [G loss: 1.350102]\n",
      "[D loss: 0.723552, acc: 42.23%] [G loss: 1.393576]\n",
      "[D loss: 0.726627, acc: 40.44%] [G loss: 1.379322]\n",
      "[D loss: 0.719302, acc: 43.63%] [G loss: 1.359547]\n",
      "[D loss: 0.701609, acc: 47.61%] [G loss: 1.396875]\n",
      "[D loss: 0.707016, acc: 43.43%] [G loss: 1.398025]\n",
      "[D loss: 0.698259, acc: 48.61%] [G loss: 1.395108]\n",
      "[D loss: 0.713192, acc: 41.24%] [G loss: 1.377781]\n",
      "[D loss: 0.712861, acc: 40.24%] [G loss: 1.380127]\n",
      "[D loss: 0.705190, acc: 41.83%] [G loss: 1.387628]\n",
      "[D loss: 0.710566, acc: 40.04%] [G loss: 1.378101]\n",
      "[D loss: 0.695575, acc: 46.02%] [G loss: 1.390242]\n",
      "[D loss: 0.695693, acc: 50.40%] [G loss: 1.390169]\n",
      "[D loss: 0.696953, acc: 48.01%] [G loss: 1.387417]\n",
      "[D loss: 0.696250, acc: 48.80%] [G loss: 1.384071]\n",
      "[D loss: 0.697985, acc: 48.21%] [G loss: 1.390102]\n",
      "[D loss: 0.693767, acc: 48.21%] [G loss: 1.392874]\n",
      "[D loss: 0.696953, acc: 42.83%] [G loss: 1.395209]\n",
      "[D loss: 0.688781, acc: 51.59%] [G loss: 1.395829]\n",
      "[D loss: 0.692677, acc: 50.20%] [G loss: 1.403464]\n",
      "[D loss: 0.695120, acc: 48.21%] [G loss: 1.393549]\n",
      "[D loss: 0.691704, acc: 51.79%] [G loss: 1.395459]\n",
      "[D loss: 0.692611, acc: 49.60%] [G loss: 1.392876]\n",
      "[D loss: 0.687075, acc: 56.77%] [G loss: 1.402495]\n",
      "[D loss: 0.690078, acc: 51.79%] [G loss: 1.398079]\n",
      "[D loss: 0.693569, acc: 48.61%] [G loss: 1.390438]\n",
      "[D loss: 0.690377, acc: 53.78%] [G loss: 1.392342]\n",
      "[D loss: 0.689072, acc: 54.38%] [G loss: 1.396103]\n",
      "[D loss: 0.691252, acc: 53.59%] [G loss: 1.405869]\n",
      "[D loss: 0.693441, acc: 48.21%] [G loss: 1.395398]\n",
      "[D loss: 0.689827, acc: 54.98%] [G loss: 1.402856]\n",
      "[D loss: 0.695807, acc: 46.22%] [G loss: 1.395577]\n",
      "[D loss: 0.690303, acc: 53.98%] [G loss: 1.403043]\n",
      "[D loss: 0.691372, acc: 51.00%] [G loss: 1.392070]\n",
      "[D loss: 0.688645, acc: 58.17%] [G loss: 1.399123]\n",
      "[D loss: 0.689058, acc: 55.78%] [G loss: 1.407754]\n",
      "[D loss: 0.689092, acc: 56.57%] [G loss: 1.400763]\n",
      "[D loss: 0.689214, acc: 56.57%] [G loss: 1.397904]\n",
      "[D loss: 0.688628, acc: 57.17%] [G loss: 1.401416]\n",
      "[D loss: 0.691872, acc: 52.79%] [G loss: 1.404150]\n",
      "[D loss: 0.690570, acc: 54.58%] [G loss: 1.406046]\n",
      "[D loss: 0.687374, acc: 56.18%] [G loss: 1.398695]\n",
      "[D loss: 0.683626, acc: 57.97%] [G loss: 1.399873]\n",
      "[D loss: 0.691121, acc: 53.59%] [G loss: 1.404506]\n",
      "[D loss: 0.685044, acc: 56.18%] [G loss: 1.405721]\n",
      "[D loss: 0.686305, acc: 57.77%] [G loss: 1.405104]\n",
      "[D loss: 0.686325, acc: 54.78%] [G loss: 1.401368]\n",
      "[D loss: 0.684452, acc: 56.77%] [G loss: 1.402283]\n",
      "[D loss: 0.691135, acc: 53.19%] [G loss: 1.408189]\n",
      "[D loss: 0.690837, acc: 53.78%] [G loss: 1.408049]\n",
      "[D loss: 0.682587, acc: 58.57%] [G loss: 1.405156]\n",
      "[D loss: 0.687448, acc: 55.58%] [G loss: 1.418645]\n",
      "[D loss: 0.685388, acc: 59.36%] [G loss: 1.413023]\n",
      "[D loss: 0.685506, acc: 55.78%] [G loss: 1.424990]\n",
      "[D loss: 0.681608, acc: 58.96%] [G loss: 1.418685]\n",
      "[D loss: 0.685210, acc: 56.97%] [G loss: 1.407845]\n",
      "[D loss: 0.685577, acc: 58.17%] [G loss: 1.423208]\n",
      "[D loss: 0.685037, acc: 60.76%] [G loss: 1.402999]\n",
      "[D loss: 0.683807, acc: 58.76%] [G loss: 1.406749]\n",
      "[D loss: 0.687494, acc: 59.16%] [G loss: 1.402241]\n",
      "[D loss: 0.688528, acc: 54.18%] [G loss: 1.412911]\n",
      "[D loss: 0.684164, acc: 61.55%] [G loss: 1.415039]\n",
      "[D loss: 0.682782, acc: 58.37%] [G loss: 1.429790]\n",
      "[D loss: 0.685021, acc: 58.76%] [G loss: 1.422324]\n",
      "[D loss: 0.687677, acc: 55.18%] [G loss: 1.422837]\n",
      "[D loss: 0.686163, acc: 56.77%] [G loss: 1.426471]\n",
      "[D loss: 0.683032, acc: 59.16%] [G loss: 1.414929]\n",
      "[D loss: 0.679405, acc: 59.16%] [G loss: 1.421763]\n",
      "[D loss: 0.682456, acc: 59.96%] [G loss: 1.418376]\n",
      "[D loss: 0.681708, acc: 59.96%] [G loss: 1.440237]\n",
      "[D loss: 0.685776, acc: 56.37%] [G loss: 1.418599]\n",
      "[D loss: 0.682328, acc: 58.57%] [G loss: 1.421124]\n",
      "[D loss: 0.685196, acc: 56.18%] [G loss: 1.429251]\n",
      "[D loss: 0.685116, acc: 57.77%] [G loss: 1.443550]\n",
      "[D loss: 0.682417, acc: 58.17%] [G loss: 1.432219]\n",
      "[D loss: 0.684578, acc: 57.77%] [G loss: 1.432477]\n",
      "[D loss: 0.679277, acc: 61.16%] [G loss: 1.425626]\n",
      "[D loss: 0.677621, acc: 62.35%] [G loss: 1.422683]\n",
      "[D loss: 0.682085, acc: 59.56%] [G loss: 1.428690]\n",
      "[D loss: 0.679687, acc: 62.75%] [G loss: 1.426932]\n",
      "[D loss: 0.690792, acc: 51.59%] [G loss: 1.418081]\n",
      "[D loss: 0.687279, acc: 57.17%] [G loss: 1.426370]\n",
      "[D loss: 0.680638, acc: 58.37%] [G loss: 1.437676]\n",
      "[D loss: 0.685116, acc: 55.98%] [G loss: 1.425324]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiGAN(batch_size=251, iterations=10000, z_size=100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_size = 100\n",
    "bigan = BiGAN(z_size=z_size,\n",
    "              batch_size=25,\n",
    "              iterations=10000)\n",
    "\n",
    "bigan.fit(X=ret_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple        0.008996\n",
       "Google      -0.005407\n",
       "Microsoft   -0.031060\n",
       "Intel       -0.025266\n",
       "Box         -0.050756\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bigan.generator.predict(x=np.array([xi]))[0] * std) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple       -0.025379\n",
       "Google       0.000997\n",
       "Microsoft    0.004552\n",
       "Intel       -0.004718\n",
       "Box         -0.075180\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_data.iloc[0] * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/plotnine/utils.py:281: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ndistinct = ids.apply(len_unique, axis=0).as_matrix()\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/plotnine/layer.py:363: UserWarning: stat_density : Removed 4749 rows containing non-finite values.\n",
      "  data = self.stat.compute_layer(data, params, layout)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/pandas/core/generic.py:4388: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/pandas/core/generic.py:4389: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n",
      "/Users/samson/anaconda3/envs/market_risk/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuAAAAGtCAYAAABX8MEmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1767cc860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (392695707)>\n"
     ]
    }
   ],
   "source": [
    "n_sim = 5000\n",
    "noise = np.random.uniform(low=-1.0, high=1.0, size=(n_sim, z_size))\n",
    "x = np.zeros((n_sim, 5))\n",
    "x_mean = np.zeros(n_sim)\n",
    "for i, xi in enumerate(noise):\n",
    "    x[i, :] = (bigan.generator.predict(x=np.array([xi]))[0] * std) + mean\n",
    "    x_mean[i] = np.average(a=x[i, :])\n",
    "\n",
    "act_mean = np.zeros(251)\n",
    "for i in range(251):\n",
    "    act_mean[i] = np.average(a=(ret_data.iloc[i] * std) + mean)\n",
    "    \n",
    "plot = ggplot(pd.melt(pd.concat([pd.DataFrame(x_mean, columns=[\"ret\"]).reset_index(drop=True), \n",
    "                                 pd.DataFrame(act_mean, columns=[\"act\"]).reset_index(drop=True)], \n",
    "                                axis=1))) + \\\n",
    "geom_density(aes(x=\"value\", \n",
    "                 fill = \"factor(variable)\"), \n",
    "             alpha=0.5, \n",
    "             color=\"black\") + \\\n",
    "theme_matplotlib()\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00527348,  0.01192187,  0.01332477, ...,  0.00384765,\n",
       "        0.00504646,  0.00293594])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.029730488623726434"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(a=x_mean, axis=0, q=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.007805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.020473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.010076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.023546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.026532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.024636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.016251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.028713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.047572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.013124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.009705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.014352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.028507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.010283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.023820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.005455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.012940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.045609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.053763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.015020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.028742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.003590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ret</td>\n",
       "      <td>-0.002485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.008598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ret</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>act</td>\n",
       "      <td>0.006675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>act</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>act</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>act</td>\n",
       "      <td>0.002526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>act</td>\n",
       "      <td>0.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>act</td>\n",
       "      <td>0.003926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.012182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.015716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>act</td>\n",
       "      <td>0.009462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.003608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>act</td>\n",
       "      <td>0.006875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>act</td>\n",
       "      <td>0.013575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>act</td>\n",
       "      <td>0.003120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>act</td>\n",
       "      <td>0.006231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>act</td>\n",
       "      <td>0.012183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.007142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>act</td>\n",
       "      <td>0.005528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.003921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>act</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>act</td>\n",
       "      <td>0.004526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.004893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.008289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>act</td>\n",
       "      <td>0.000311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>act</td>\n",
       "      <td>0.004371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>act</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>act</td>\n",
       "      <td>-0.009391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    variable     value\n",
       "0        ret  0.007805\n",
       "1        ret -0.020473\n",
       "2        ret -0.010076\n",
       "3        ret -0.023546\n",
       "4        ret  0.026532\n",
       "5        ret  0.024636\n",
       "6        ret -0.016251\n",
       "7        ret  0.028713\n",
       "8        ret -0.047572\n",
       "9        ret -0.010905\n",
       "10       ret  0.013124\n",
       "11       ret  0.009705\n",
       "12       ret -0.002892\n",
       "13       ret -0.014352\n",
       "14       ret  0.028507\n",
       "15       ret -0.010283\n",
       "16       ret -0.023820\n",
       "17       ret  0.005455\n",
       "18       ret  0.012940\n",
       "19       ret -0.045609\n",
       "20       ret -0.053763\n",
       "21       ret  0.015020\n",
       "22       ret -0.028742\n",
       "23       ret -0.037189\n",
       "24       ret  0.003590\n",
       "25       ret  0.009704\n",
       "26       ret -0.003209\n",
       "27       ret -0.002485\n",
       "28       ret  0.008598\n",
       "29       ret  0.000818\n",
       "..       ...       ...\n",
       "472      act  0.006675\n",
       "473      act  0.000138\n",
       "474      act  0.006870\n",
       "475      act  0.002526\n",
       "476      act -0.003623\n",
       "477      act  0.001287\n",
       "478      act  0.003926\n",
       "479      act -0.001673\n",
       "480      act -0.012182\n",
       "481      act -0.015716\n",
       "482      act  0.009462\n",
       "483      act -0.003608\n",
       "484      act  0.006875\n",
       "485      act  0.013575\n",
       "486      act  0.003120\n",
       "487      act  0.006231\n",
       "488      act -0.004682\n",
       "489      act  0.012183\n",
       "490      act -0.007142\n",
       "491      act  0.005528\n",
       "492      act -0.003921\n",
       "493      act  0.007552\n",
       "494      act  0.004526\n",
       "495      act -0.004893\n",
       "496      act -0.008289\n",
       "497      act  0.000311\n",
       "498      act  0.004371\n",
       "499      act -0.007254\n",
       "500      act  0.000405\n",
       "501      act -0.009391\n",
       "\n",
       "[502 rows x 2 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.melt(pd.concat([pd.DataFrame(x_mean, columns=[\"ret\"]).reset_index(drop=True), pd.DataFrame(act_mean, columns=[\"act\"]).reset_index(drop=True)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n",
    "2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n",
    "3. Kingma, D. P., and Welling M. (2014). Auto-Encoding Variational Bayes (https://arxiv.org/abs/1312.6114).\n",
    "4. http://scikit-learn.org/stable/#\n",
    "5. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "6. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "7. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "8. https://keras.io\n",
    "9. Chollet, F. (2018). Deep Learning with Python (Manning).\n",
    "10. Hull, John C. (2010). Risk Management and Financial Institutions (Pearson).\n",
    "11. https://towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351\n",
    "12. https://towardsdatascience.com/automatic-feature-engineering-using-generative-adversarial-networks-8e24b3c16bf3\n",
    "13. Donahue, J., KrÃ¤henbÃ¼hl, P. and Darrell, T. (2017). Adversarial Feature Learning (https://arxiv.org/pdf/1605.09782).\n",
    "14. https://github.com/eriklindernoren/Keras-GAN"
   ]
  }
 ],
 "metadata": {
  "author": "mes",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
